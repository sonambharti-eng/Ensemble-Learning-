{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Theoretical Questions :-"
      ],
      "metadata": {
        "id": "B3V0Dgi1JmRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.1 Can we use Bagging for regression problems?"
      ],
      "metadata": {
        "id": "t9i6F60eLFMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.1 Yes, **Bagging** (Bootstrap Aggregating) can definitely be used for **regression problems**. In fact, Bagging is often applied to regression tasks, not just classification.\n",
        "\n",
        "Here’s how it works for regression:\n",
        "1. **Multiple models** (e.g., decision trees) are trained on different subsets of the data generated by bootstrapping (random sampling with replacement).\n",
        "2. Each individual model makes its own prediction for new data.\n",
        "3. The final prediction is typically the **average** of the predictions from all the models.\n",
        "\n",
        "For example, **Random Forests** is a popular ensemble method for regression that uses bagging with decision trees as the base learners. It aggregates the predictions by averaging them to get the final output.\n",
        "\n",
        "So, Bagging can help reduce variance and improve the stability of regression models, particularly when using models like decision trees, which tend to have high variance."
      ],
      "metadata": {
        "id": "_7BMxW7PLlyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.2  What is the difference between multiple model training and single model training?"
      ],
      "metadata": {
        "id": "uCP7qbh2MevM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.2 The main difference between **multiple model training** and **single model training** lies in how the models are used and the training approach:\n",
        "\n",
        "### **Single Model Training**:\n",
        "- **Definition**: In single model training, you train a single machine learning model to solve a specific problem or task.\n",
        "- **Process**: The training process involves feeding data into a single algorithm or architecture, optimizing its parameters based on the training data, and evaluating it on validation or test data.\n",
        "- **Use Case**: This approach is commonly used for tasks where a single model is sufficient to perform the desired function, such as classification, regression, or object detection.\n",
        "- **Pros**:\n",
        "  - Simpler and faster to implement.\n",
        "  - Easier to deploy and maintain.\n",
        "- **Cons**:\n",
        "  - May not perform well if the task requires complex or multi-faceted understanding (e.g., in the case of multiple languages, different data types, or highly complex tasks).\n",
        "\n",
        "### **Multiple Model Training**:\n",
        "- **Definition**: Multiple model training involves training multiple models simultaneously or sequentially, either independently or in conjunction with each other, to solve a particular problem.\n",
        "- **Process**: The models might be trained on different subsets of data, use different algorithms, or have different architectures. The outputs from multiple models can be combined in various ways, such as through voting, stacking, or ensemble methods.\n",
        "- **Use Case**: Common in situations where different models specialize in different aspects of a problem, or when combining multiple models can increase accuracy and robustness. For example, ensemble methods like Random Forests or XGBoost, which combine several decision trees, or multi-task learning, where different models are trained for related tasks.\n",
        "- **Pros**:\n",
        "  - Can improve accuracy and performance by leveraging the strengths of multiple models.\n",
        "  - Better suited for complex tasks or tasks with diverse data types (e.g., images, text, and time series data).\n",
        "- **Cons**:\n",
        "  - More computationally expensive and complex to manage.\n",
        "  - Requires more resources for training, tuning, and deployment.\n",
        "\n",
        "In short, **single model training** is straightforward and works well for simpler tasks, while **multiple model training** can offer enhanced performance by combining the strengths of different models but is more complex and resource-intensive."
      ],
      "metadata": {
        "id": "yk_7Bxj_NC6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.3 Explain the concept of feature randomness in Random Forest.\n"
      ],
      "metadata": {
        "id": "gO6h8wHZDWkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.3 In a Random Forest, **feature randomness** refers to the process of randomly selecting a subset of features (or variables) at each decision tree node, rather than considering all features when splitting a node. This randomness is a key element in the construction of Random Forests and contributes to its power and effectiveness.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1. **Training the Decision Trees**: Random Forest builds multiple decision trees, where each tree is trained on a random subset of the training data (through bootstrapping, meaning sampling with replacement).\n",
        "\n",
        "2. **Random Feature Selection**: At each node of a tree, instead of considering all the available features to decide the best split, only a random subset of features is considered. This is typically a smaller number than the total number of features, often the square root of the total number of features for classification tasks.\n",
        "\n",
        "3. **Why It Works**: The idea behind feature randomness is to introduce diversity among the individual trees. By limiting the number of features each tree can consider, it reduces the correlation between the trees. This helps prevent overfitting and improves the generalization ability of the model. It also makes the model more robust, as individual trees might overfit on certain features, but the ensemble (the Random Forest) will average out errors.\n",
        "\n",
        "4. **Overall Effect**: Feature randomness helps Random Forests become a more powerful model because it combines the strength of multiple, diverse decision trees, each focusing on different aspects of the data. This \"diversity\" in the trees leads to a more accurate and stable model when predictions are made.\n",
        "\n",
        "In summary, feature randomness is a technique that helps enhance the performance and robustness of a Random Forest model by randomly selecting a subset of features at each split in each decision tree, leading to less overfitting and improved generalization.\n"
      ],
      "metadata": {
        "id": "mPJ7yM7EEgb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.4 What is OOB (Out-of-Bag) Score?"
      ],
      "metadata": {
        "id": "HrYuQs8VE27L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS. The **Out-of-Bag (OOB) score** is a concept used in ensemble learning methods, particularly in **Random Forests**. It provides an estimate of the model’s performance without needing a separate validation set.\n",
        "\n",
        "### How OOB Score Works:\n",
        "\n",
        "1. **Bootstrap Sampling**: In Random Forests, each tree is trained on a bootstrapped subset of the data (i.e., a random sample with replacement). For each tree, roughly **one-third** of the data points are not used for training that specific tree. These points are called **out-of-bag**.\n",
        "\n",
        "2. **Prediction with OOB Data**: Once all trees are trained, the out-of-bag data points (those not included in the training set for a specific tree) are used to test the model. This means that for each data point, you can gather predictions from all the trees where that data point was not included in the training process.\n",
        "\n",
        "3. **OOB Score Calculation**: The OOB score is computed by averaging the accuracy of the predictions made for the out-of-bag data across all trees. It is essentially an internal cross-validation method.\n",
        "\n",
        "### Why Use OOB Score?\n",
        "\n",
        "- **No Need for a Validation Set**: Since OOB provides a performance estimate from data that wasn't used in training the trees, you don't need a separate validation set. This is particularly useful when data is limited.\n",
        "- **Efficient**: It helps make the most of the available data by using it both for training and testing.\n",
        "- **Less Bias**: It helps to get a more unbiased estimate of model performance.\n",
        "\n",
        "### Formula:\n",
        "For classification, the OOB score can be computed as:\n",
        "\\[\n",
        "\\text{OOB Accuracy} = \\frac{\\text{Number of Correct Predictions on OOB Data}}{\\text{Total Number of OOB Predictions}}\n",
        "\\]\n",
        "\n",
        "In summary, the OOB score is a valuable technique for evaluating the performance of a Random Forest model without needing a dedicated test set."
      ],
      "metadata": {
        "id": "c0Dg6kGZFelj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.5 How can you measure the importance of features in a Random Forest model?"
      ],
      "metadata": {
        "id": "ZxLbtCqKFzZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hozTNTN9H1FO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.5 In a Random Forest model, you can measure the importance of features using various methods. The most common and straightforward ways are:\n",
        "\n",
        "### 1. **Mean Decrease Impurity (Gini Impurity or Entropy)**\n",
        "   - This is the most widely used method for feature importance in Random Forests.\n",
        "   - **How it works:** Each tree in the Random Forest splits the data based on certain features. When a feature is used to split the data, the impurity (Gini Impurity or Entropy) is reduced. The more a feature helps to reduce the impurity (i.e., the more \"pure\" the nodes become), the more important that feature is.\n",
        "   - **Output:** A higher decrease in impurity indicates a higher importance of the feature.\n",
        "   \n",
        "   In Python's `sklearn`, you can get feature importance using `model.feature_importances_` after fitting the Random Forest model:\n",
        "   ```python\n",
        "   from sklearn.ensemble import RandomForestClassifier\n",
        "   \n",
        "   model = RandomForestClassifier()\n",
        "   model.fit(X_train, y_train)\n",
        "   feature_importances = model.feature_importances_\n",
        "   ```\n",
        "\n",
        "### 2. **Mean Decrease Accuracy (Permutation Importance)**\n",
        "   - **How it works:** This method involves randomly shuffling the values of each feature and observing the change in model performance. If a feature is important, permuting its values will lead to a significant drop in the model's accuracy. If permuting a feature does not affect the model’s performance, the feature is less important.\n",
        "   - **Implementation:** You can use permutation importance in Python using libraries like `sklearn` or `eli5`. This method is model-agnostic and can be used for any machine learning model.\n",
        "   \n",
        "   Example using `sklearn`:\n",
        "   ```python\n",
        "   from sklearn.inspection import permutation_importance\n",
        "   \n",
        "   result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
        "   perm_importance = result.importances_mean\n",
        "   ```\n",
        "\n",
        "### 3. **SHAP (SHapley Additive exPlanations) Values**\n",
        "   - SHAP values provide a more detailed and interpretable way to understand the contribution of each feature to individual predictions. This method is based on cooperative game theory and calculates how much each feature contributes to the prediction relative to the baseline.\n",
        "   - **How it works:** It calculates the contribution of each feature for each prediction by considering all possible permutations of features. The results are averaged over all predictions to provide a global interpretation.\n",
        "   - **Implementation:** You can use the `shap` library to compute SHAP values for a Random Forest model.\n",
        "   \n",
        "   Example using `shap`:\n",
        "   ```python\n",
        "   import shap\n",
        "   \n",
        "   explainer = shap.TreeExplainer(model)\n",
        "   shap_values = explainer.shap_values(X_train)\n",
        "   shap.summary_plot(shap_values, X_train)\n",
        "   ```\n",
        "\n",
        "### 4. **Tree-Based Feature Importance (using Sklearn's `RandomForestClassifier`/`RandomForestRegressor`)**\n",
        "   - In Random Forests, each individual tree calculates feature importance based on how often a feature is used in splits and how much it helps reduce impurity.\n",
        "   - **How it works:** The average feature importance across all trees in the forest is calculated and used as a global measure of feature importance.\n",
        "\n",
        "### Summary\n",
        "- **Mean Decrease Impurity (Gini/Entropy)**: Measures how often a feature is used for splits, weighted by the impurity reduction.\n",
        "- **Permutation Importance (Mean Decrease Accuracy)**: Measures the effect of random feature shuffling on model performance.\n",
        "- **SHAP Values**: Provides detailed contributions of each feature to each individual prediction.\n",
        "\n",
        "Each of these methods has its strengths, and the best one depends on the context and what you need from the feature importance (e.g., global vs. local interpretability)."
      ],
      "metadata": {
        "id": "XSdcb79DHL8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.6 Explain the working principle of a Bagging Classifier?"
      ],
      "metadata": {
        "id": "k6oSn3iCL3K3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.6 The **working principle of a Bagging Classifier** (short for **Bootstrap Aggregating**) is an ensemble method that improves the performance and stability of machine learning algorithms, particularly those that are sensitive to variations in the data, like decision trees.\n",
        "\n",
        "Here’s how it works:\n",
        "\n",
        "1. **Bootstrapping**:\n",
        "   - First, multiple subsets of the training data are created by **random sampling** with replacement (this is called **bootstrapping**).\n",
        "   - Each subset will be of the same size as the original dataset, but some data points may appear multiple times, while others may be left out.\n",
        "\n",
        "2. **Model Training**:\n",
        "   - A separate classifier (often a decision tree, but it can be any machine learning model) is trained on each of the bootstrapped datasets.\n",
        "   - The idea is that by training multiple models on different subsets of data, each model learns a slightly different representation of the data.\n",
        "\n",
        "3. **Prediction**:\n",
        "   - For classification tasks, when it comes time to make a prediction on new, unseen data, each model (trained on different subsets) makes its own prediction.\n",
        "   - **Majority Voting** is used to combine the predictions of all the individual models. The final predicted class is the one that gets the most votes from all the classifiers.\n",
        "\n",
        "4. **Result**:\n",
        "   - Bagging reduces variance and helps prevent overfitting because it averages out the predictions of the individual models. By training on different data subsets, bagging effectively smoothens out any noise or overfitting tendencies that might arise from any single model.\n",
        "\n",
        "### Key Points:\n",
        "- **Reduction in variance**: By using multiple models and averaging their predictions, Bagging reduces the overall variance, making the model more robust to fluctuations in the data.\n",
        "- **Improvement over weak models**: It is particularly useful when the base model is a weak learner (e.g., decision trees).\n",
        "- **Example**: A common implementation of Bagging is **Random Forests**, where decision trees are used as base models and combined using Bagging.\n",
        "\n",
        "Overall, the Bagging Classifier leverages the strength of multiple classifiers to provide more accurate and stable predictions."
      ],
      "metadata": {
        "id": "lBY0Ldm6Nd5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.7 How do you evaluate a Bagging Classifier’s performance?"
      ],
      "metadata": {
        "id": "ycFffkM1N35-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.7 Evaluating the performance of a Bagging Classifier (or any machine learning model) generally involves several key steps. Here are the common methods:\n",
        "\n",
        "### 1. **Accuracy**\n",
        "   - **Definition**: Accuracy is the proportion of correctly predicted instances out of the total instances.\n",
        "   - **How to use**: Calculate the percentage of correct predictions versus the total number of predictions made. While it's a common metric, it may not be sufficient if the dataset is imbalanced.\n",
        "   - **Formula**:\n",
        "     \\[\n",
        "     \\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}} \\times 100\n",
        "     \\]\n",
        "\n",
        "### 2. **Confusion Matrix**\n",
        "   - **Definition**: The confusion matrix is a table that compares the predicted labels to the actual labels, providing a detailed view of the classifier's performance. It helps evaluate the classifier’s performance in different classes (for multi-class classification).\n",
        "   - **How to use**: From the confusion matrix, you can derive metrics like precision, recall, and F1-score.\n",
        "   - **Components**: True positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
        "\n",
        "### 3. **Precision, Recall, and F1-Score**\n",
        "   - **Precision**: The proportion of positive predictions that were actually correct.\n",
        "     \\[\n",
        "     \\text{Precision} = \\frac{TP}{TP + FP}\n",
        "     \\]\n",
        "   - **Recall**: The proportion of actual positives that were correctly predicted.\n",
        "     \\[\n",
        "     \\text{Recall} = \\frac{TP}{TP + FN}\n",
        "     \\]\n",
        "   - **F1-Score**: The harmonic mean of precision and recall, balancing the two metrics.\n",
        "     \\[\n",
        "     \\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "     \\]\n",
        "\n",
        "### 4. **Cross-Validation**\n",
        "   - **Definition**: Cross-validation involves splitting the dataset into multiple subsets, training the model on some subsets and testing on the remaining ones to evaluate performance more robustly.\n",
        "   - **How to use**: Commonly, **k-fold cross-validation** is used where the dataset is divided into **k** parts, and each part is used as a test set while the model is trained on the remaining k-1 parts. The average performance across all folds is then reported.\n",
        "   - **Why it's useful**: It reduces overfitting by giving you a better estimate of model performance on unseen data.\n",
        "\n",
        "### 5. **Out-of-Bag (OOB) Error Estimate**\n",
        "   - **Definition**: One of the unique features of Bagging classifiers (such as Random Forest) is the use of out-of-bag (OOB) samples. When training a Bagging model, each base model is trained on a bootstrap sample of the data, meaning that some samples are left out of the training set for each base model. These left-out samples can be used to estimate the generalization error (OOB error).\n",
        "   - **How to use**: The OOB error can be computed similarly to cross-validation, where the predictions for each OOB sample are aggregated to estimate the error.\n",
        "   - **Why it's useful**: OOB error gives a good approximation of model performance without needing a separate validation set.\n",
        "\n",
        "### 6. **Receiver Operating Characteristic (ROC) Curve and AUC**\n",
        "   - **ROC Curve**: A graphical plot that shows the performance of a classifier at all classification thresholds. It is a plot of the true positive rate (sensitivity) versus the false positive rate (1 - specificity).\n",
        "   - **AUC (Area Under the Curve)**: The area under the ROC curve. The higher the AUC, the better the model's ability to distinguish between the classes.\n",
        "\n",
        "### 7. **Learning Curves**\n",
        "   - **Definition**: Learning curves plot the performance of the model (typically accuracy or error rate) as a function of training size or training time.\n",
        "   - **How to use**: By plotting the training and validation errors against training size, you can detect overfitting, underfitting, or whether the model needs more data.\n",
        "\n",
        "### 8. **Model Hyperparameters Tuning**\n",
        "   - **Definition**: Evaluating performance after tuning hyperparameters like the number of base models (estimators), the number of features considered in each model, and the maximum depth of trees (for decision tree-based bagging classifiers).\n",
        "   - **How to use**: This is typically done through grid search or random search with cross-validation to find the optimal set of hyperparameters for better performance.\n",
        "\n",
        "---\n",
        "\n",
        "By using these methods together, you can comprehensively evaluate the performance of a Bagging Classifier and gain insights into its strengths and weaknesses."
      ],
      "metadata": {
        "id": "Oov3KVWNOGOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.8 How does a Bagging Regressor work?"
      ],
      "metadata": {
        "id": "sERiiRk9PlJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.8 A **Bagging Regressor** is an ensemble learning method that uses the **bagging (Bootstrap Aggregating)** technique to improve the accuracy and stability of regression models. It works by combining predictions from multiple models to reduce variance and prevent overfitting. Here’s how it works step-by-step:\n",
        "\n",
        "1. **Bootstrap Sampling (Data Sampling)**:\n",
        "   - The algorithm begins by creating multiple subsets of the original training data. Each subset is created by randomly sampling data points **with replacement** from the original dataset. This means some data points can be repeated in a subset, while others may be left out.\n",
        "\n",
        "2. **Training Multiple Models**:\n",
        "   - A regression model (e.g., decision trees, linear regression) is trained on each of these bootstrap samples. These models can be the same type (e.g., all decision trees), but since the training data for each model is different, each model might learn slightly different patterns.\n",
        "\n",
        "3. **Prediction**:\n",
        "   - Once all the models are trained, predictions are made by each individual model. Since Bagging is an ensemble method, predictions from multiple models are averaged to get the final output. In the case of regression, the final prediction is the **average of the individual model predictions**.\n",
        "\n",
        "4. **Reduction of Variance**:\n",
        "   - By combining predictions from several models trained on different data subsets, Bagging reduces the variance of the model. Each individual model may have high variance (due to overfitting), but averaging the predictions of many models helps to smooth out the errors, resulting in a more stable and robust prediction.\n",
        "\n",
        "### Key Benefits of Bagging Regressor:\n",
        "- **Reduces overfitting**: By averaging the results from multiple models, Bagging helps in reducing overfitting, especially when the base model is prone to it, like decision trees.\n",
        "- **Improves accuracy**: Bagging usually results in more accurate predictions compared to a single model.\n",
        "- **Works well with high variance models**: It is particularly effective for models that have high variance, such as decision trees.\n",
        "\n",
        "### Example:\n",
        "Imagine you have a dataset with housing prices, and you want to predict the price of a house given various features (e.g., size, location). By applying Bagging, you would create multiple subsets of the data, train a regression model (like decision trees) on each subset, and then average the predictions from all the models to get the final predicted price.\n",
        "\n",
        "In practice, one popular implementation of a Bagging Regressor is the **BaggingRegressor** in scikit-learn, where a default base model is typically a decision tree regressor.\n",
        "\n",
        "Does that clarify how it works for you? Let me know if you'd like more details!"
      ],
      "metadata": {
        "id": "yxjsyonQP-iC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.9 What is the main advantage of ensemble techniques?"
      ],
      "metadata": {
        "id": "A4wszSY4QVRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.9 The main advantage of ensemble techniques is that they combine multiple models to make better predictions than any single model on its own. By leveraging the strengths and mitigating the weaknesses of individual models, ensemble methods can reduce errors and improve overall performance, leading to better accuracy, generalization, and robustness.\n",
        "\n",
        "There are several specific benefits:\n",
        "1. **Improved accuracy**: Combining models typically leads to better overall performance, as the weaknesses of one model can be counterbalanced by others.\n",
        "2. **Reduction of overfitting**: By using different models, ensemble methods can reduce the risk of overfitting compared to a single complex model.\n",
        "3. **Stability**: They can make predictions that are less sensitive to small changes in the data, leading to more consistent results.\n",
        "4. **Handling biases**: Different models might have different biases, and ensembles can help correct these by combining diverse perspectives.\n",
        "\n",
        "Popular ensemble techniques include **bagging** (e.g., Random Forest), **boosting** (e.g., Gradient Boosting, AdaBoost), and **stacking**. Each has its own way of combining predictions to improve performance."
      ],
      "metadata": {
        "id": "W7edENiaQf7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.10 What is the main challenge of ensemble methods?"
      ],
      "metadata": {
        "id": "PtiJyTn8QxIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main challenge of ensemble methods is **overfitting**, particularly when the base models are complex. While ensemble methods, like bagging and boosting, aim to improve model performance by combining multiple learners, they can still lead to overfitting in certain cases, especially when:\n",
        "\n",
        "1. **Base models are overly complex**: If each individual model in the ensemble is highly complex (e.g., deep decision trees), the ensemble might learn too much from the training data, capturing noise rather than general patterns.\n",
        "   \n",
        "2. **Too many base models**: Including too many base models without sufficient regularization can cause the ensemble to overfit, particularly if the models are not diverse enough.\n",
        "\n",
        "3. **Poor generalization**: If the models in the ensemble are not diverse or are highly correlated, the ensemble may not improve much over a single model, and the predictions may still be prone to overfitting.\n",
        "\n",
        "To mitigate this, strategies like cross-validation, proper model regularization, and ensuring diversity among the base models are typically employed."
      ],
      "metadata": {
        "id": "AXhQ5qU2SEe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.11 Explain the key idea behind ensemble techniques?"
      ],
      "metadata": {
        "id": "GmSOhON-RH8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.11 Ensemble techniques are machine learning methods that combine multiple individual models to improve the overall performance of a predictive system. The key idea behind ensemble techniques is that by combining the strengths of different models, we can reduce the risk of errors and increase accuracy, robustness, and generalization ability compared to using a single model.\n",
        "\n",
        "The main principles behind ensemble methods include:\n",
        "\n",
        "1. **Diversity of Models**: By using different models (e.g., decision trees, neural networks, etc.) or variations of the same model (e.g., with different training data or hyperparameters), ensemble methods create diversity, which helps in making more accurate predictions.\n",
        "\n",
        "2. **Aggregation of Predictions**: Once different models are trained, their predictions are aggregated in some way to form a final decision. Common methods of aggregation include:\n",
        "   - **Voting**: For classification tasks, each model \"votes\" on the predicted class, and the majority vote becomes the final prediction (e.g., in Random Forests).\n",
        "   - **Averaging**: For regression tasks, predictions from all models are averaged to form a final result (e.g., in Bagging methods like Random Forest).\n",
        "\n",
        "3. **Reduction of Overfitting**: Ensembles can help reduce the variance and bias in the model. They smooth out the errors of individual models, leading to a more robust overall model. This helps prevent overfitting to the training data.\n",
        "\n",
        "4. **Improved Performance**: By combining multiple models, ensemble methods often lead to better performance in terms of accuracy, precision, and recall, especially in complex or noisy data scenarios.\n",
        "\n",
        "Common types of ensemble techniques include:\n",
        "- **Bagging**: Builds multiple models independently (e.g., Random Forest).\n",
        "- **Boosting**: Builds models sequentially, with each new model focusing on the errors made by the previous one (e.g., AdaBoost, Gradient Boosting).\n",
        "- **Stacking**: Combines predictions from different models by training a meta-model on the outputs of the base models.\n",
        "\n",
        "Overall, the strength of ensemble methods lies in their ability to leverage multiple models to enhance prediction accuracy and resilience."
      ],
      "metadata": {
        "id": "Wd_7-2pSRVAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.12 What is a Random Forest Classifier?"
      ],
      "metadata": {
        "id": "uBhSQpGfSOog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.12 A **Random Forest Classifier** is an ensemble learning algorithm used for classification tasks in machine learning. It builds multiple decision trees during training and merges their outputs to make a final classification decision. The main idea behind a random forest is to combine the results of several decision trees to improve accuracy and prevent overfitting, which is common in single decision trees.\n",
        "\n",
        "Here’s a breakdown of how it works:\n",
        "\n",
        "1. **Bootstrap Sampling**: The algorithm generates multiple subsets of the training data by randomly selecting samples with replacement. This technique is called *bootstrap aggregating* (or *bagging*).\n",
        "\n",
        "2. **Decision Trees**: For each subset, a decision tree is trained. However, instead of considering all features at each node, only a random subset of features is considered to split each node. This helps in reducing the correlation between the individual trees and increases diversity within the forest.\n",
        "\n",
        "3. **Voting**: After the trees are built, each tree makes its own prediction for the class label of a sample. The Random Forest Classifier then takes a *majority vote* from all the trees to determine the final class label for the sample.\n",
        "\n",
        "### Advantages:\n",
        "- **Robustness**: Random forests are less prone to overfitting than a single decision tree.\n",
        "- **Accuracy**: They typically perform well on both small and large datasets.\n",
        "- **Feature Importance**: Random Forest can also give insights into the importance of features used in making predictions.\n",
        "\n",
        "### Disadvantages:\n",
        "- **Complexity**: The model can be more computationally expensive to train and slower to predict because it involves many decision trees.\n",
        "- **Interpretability**: Unlike a single decision tree, a random forest is harder to interpret because it combines many trees.\n",
        "\n",
        "In short, a Random Forest Classifier is a powerful tool for classification problems, leveraging multiple decision trees to create more accurate and stable predictions."
      ],
      "metadata": {
        "id": "QGdq3x2HS1fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.13 What are the main types of ensemble techniques?"
      ],
      "metadata": {
        "id": "cEfYxBfiS8_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.13 Ensemble techniques combine multiple models to improve overall performance by leveraging their collective strengths. The main types of ensemble techniques in machine learning are:\n",
        "\n",
        "### 1. **Bagging (Bootstrap Aggregating)**\n",
        "   - **Definition**: Bagging involves training multiple models (typically of the same type) on different random subsets of the training data and then combining their predictions.\n",
        "   - **Key Characteristics**:\n",
        "     - Models are trained independently on different bootstrapped subsets of the data.\n",
        "     - The final prediction is made by aggregating the individual models’ predictions, often through majority voting (for classification) or averaging (for regression).\n",
        "   - **Example**: **Random Forests** (a bagging method using decision trees).\n",
        "   \n",
        "### 2. **Boosting**\n",
        "   - **Definition**: Boosting builds models sequentially, where each model attempts to correct the errors made by the previous ones.\n",
        "   - **Key Characteristics**:\n",
        "     - Models are trained one at a time, with each subsequent model focusing more on the instances that were misclassified by the previous model.\n",
        "     - The final prediction is typically a weighted combination of all models.\n",
        "   - **Examples**:\n",
        "     - **AdaBoost** (Adaptive Boosting)\n",
        "     - **Gradient Boosting** (including **XGBoost**, **LightGBM**, and **CatBoost**)\n",
        "\n",
        "### 3. **Stacking (Stacked Generalization)**\n",
        "   - **Definition**: Stacking involves training multiple models (called base models) on the same dataset and then using another model (called a meta-model or blender) to combine the predictions of the base models.\n",
        "   - **Key Characteristics**:\n",
        "     - The base models can be of different types (e.g., decision trees, support vector machines, etc.).\n",
        "     - The meta-model learns how to best combine the predictions from the base models to improve overall performance.\n",
        "   - **Example**: Stacking can combine models like Random Forests, Logistic Regression, and Neural Networks, and then use another model (like a logistic regression) to make the final prediction.\n",
        "\n",
        "### 4. **Voting**\n",
        "   - **Definition**: Voting is a simple ensemble technique where multiple models (often of different types) make predictions, and the final prediction is determined by a majority vote (for classification) or averaging (for regression).\n",
        "   - **Key Characteristics**:\n",
        "     - **Hard Voting**: Each model votes for a class label, and the class with the majority of votes is chosen.\n",
        "     - **Soft Voting**: Models predict class probabilities, and the final prediction is based on the average probability for each class.\n",
        "   - **Example**: Combining predictions from a Random Forest, a Logistic Regression, and a Support Vector Machine.\n",
        "\n",
        "### 5. **Bagged Boosting**\n",
        "   - **Definition**: A hybrid technique combining both bagging and boosting strategies.\n",
        "   - **Key Characteristics**:\n",
        "     - Models are built with boosting, but the data is sampled with bagging methods to improve diversity.\n",
        "     - This can provide a balance between the strengths of both methods.\n",
        "   - **Example**: **Gradient Boosting with Random Forest as base learners**.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary:\n",
        "\n",
        "- **Bagging** reduces variance by averaging predictions (e.g., Random Forest).\n",
        "- **Boosting** reduces bias by focusing on hard-to-classify instances (e.g., AdaBoost, Gradient Boosting).\n",
        "- **Stacking** combines multiple model predictions using a meta-model (e.g., stacking decision trees with logistic regression).\n",
        "- **Voting** combines predictions based on majority rule or probabilities (e.g., majority voting between models)."
      ],
      "metadata": {
        "id": "LtGtHjSeTJpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.14 What is ensemble learning in machine learning?\n"
      ],
      "metadata": {
        "id": "5UH4n6RGThgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.14 **Ensemble learning** in machine learning is a technique where multiple models (also called \"learners\") are trained to solve the same problem, and their predictions are combined to produce a final result. The main goal of ensemble learning is to improve the performance and robustness of a machine learning model by leveraging the strengths of several individual models, rather than relying on a single model.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Combining Multiple Models**: Ensemble learning combines multiple models to make a final decision, often leading to better accuracy and generalization than individual models.\n",
        "- **Diversity of Models**: The models in an ensemble may be of the same type (e.g., decision trees) or different types (e.g., decision trees, support vector machines, and neural networks), and they are combined in a way that improves overall prediction performance.\n",
        "\n",
        "### Advantages of Ensemble Learning:\n",
        "1. **Improved Accuracy**: By combining multiple models, ensemble learning can produce more accurate predictions than any single model.\n",
        "2. **Reduced Overfitting**: Ensemble methods often reduce the risk of overfitting compared to individual models, especially when using diverse models.\n",
        "3. **Better Generalization**: Ensembles help in improving the model's ability to generalize to new, unseen data.\n",
        "4. **Robustness**: It can be more robust to noisy data and outliers.\n",
        "\n",
        "### Types of Ensemble Learning:\n",
        "- **Bagging**: Models are trained independently on different subsets of the training data, and the final prediction is made by aggregating the results (e.g., Random Forest).\n",
        "- **Boosting**: Models are trained sequentially, with each model trying to correct the mistakes of the previous one (e.g., AdaBoost, Gradient Boosting).\n",
        "- **Stacking**: Multiple different models are trained, and a meta-model is used to combine their predictions (e.g., using a logistic regression model to combine predictions from decision trees, SVMs, and others).\n",
        "- **Voting**: Multiple models are used, and the final prediction is made by majority voting (for classification) or averaging (for regression).\n",
        "\n",
        "### Common Ensemble Methods:\n",
        "- **Random Forest** (bagging with decision trees)\n",
        "- **AdaBoost** (boosting with weak learners)\n",
        "- **Gradient Boosting** (sequentially improving models by focusing on errors)\n",
        "- **XGBoost** / **LightGBM** / **CatBoost** (efficient implementations of gradient boosting)\n",
        "- **Voting Classifier** (simple voting among different models)\n",
        "\n",
        "### Summary:\n",
        "Ensemble learning is a powerful technique in machine learning that combines the predictions of multiple models to improve accuracy, reduce overfitting, and increase robustness. It works by capitalizing on the strength of different models to create a more reliable and accurate predictive system."
      ],
      "metadata": {
        "id": "naP3A2OCT3f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.15 When should we avoid using ensemble methods?"
      ],
      "metadata": {
        "id": "WlGpN0mUUQHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.15 While ensemble methods are powerful tools in machine learning, there are certain scenarios where their use might not be ideal. Here are some situations when you might want to avoid using ensemble methods:\n",
        "\n",
        "### 1. **When the Problem is Simple**\n",
        "   - **Scenario**: If you are working on a relatively simple problem where a single model (e.g., a decision tree or logistic regression) can already perform well and achieve high accuracy, ensemble methods might not add much value.\n",
        "   - **Reason to Avoid**: Ensemble methods add complexity and computational cost, which may not be justified if a simple model is already giving good performance.\n",
        "\n",
        "### 2. **When Computational Resources are Limited**\n",
        "   - **Scenario**: Ensemble methods, especially those like **Random Forests** and **Boosting**, can be computationally expensive because they involve training multiple models.\n",
        "   - **Reason to Avoid**: If you have limited computational resources (e.g., memory or processing power) or if you're working with large datasets, ensemble methods may not be efficient. They can significantly increase training time and require more memory.\n",
        "\n",
        "### 3. **When Interpretability is Critical**\n",
        "   - **Scenario**: If you need a model that is easy to interpret and explain, such as in regulated industries (e.g., healthcare, finance), simple models like **decision trees** or **logistic regression** are preferred.\n",
        "   - **Reason to Avoid**: Ensemble methods like **Random Forests** and **Gradient Boosting** involve combining multiple models, which can make the final model difficult to interpret. This is problematic when you need clear, understandable reasoning for the model’s decisions.\n",
        "\n",
        "### 4. **When the Data is Small or Noisy**\n",
        "   - **Scenario**: If you have a small dataset or noisy data, ensemble methods might overfit or fail to generalize well.\n",
        "   - **Reason to Avoid**: Ensemble methods like **Boosting** are sensitive to noisy data and can easily overfit when there's not enough data to create diverse and reliable models. In small datasets, even bagging methods might not reduce variance effectively.\n",
        "\n",
        "### 5. **When Model Simplicity and Speed Matter**\n",
        "   - **Scenario**: In real-time applications or systems where speed and simplicity are paramount (e.g., some embedded systems or mobile applications), using a complex ensemble method might not be feasible.\n",
        "   - **Reason to Avoid**: Ensemble methods like **Boosting** or **Random Forests** require significant time for training and prediction. If the problem demands fast response times or low-latency predictions, a simpler model may be preferable.\n",
        "\n",
        "### 6. **When Overfitting is a Concern**\n",
        "   - **Scenario**: Even though ensemble methods help to reduce overfitting in many cases, certain types, like **Boosting** (especially **AdaBoost**), can still overfit when the models are too complex or the data is noisy.\n",
        "   - **Reason to Avoid**: If overfitting is a major concern, and you don’t have enough data to properly train multiple models, using an ensemble method could exacerbate this issue, especially if model tuning is not carefully done.\n",
        "\n",
        "### 7. **When you Already Have a High-performing Model**\n",
        "   - **Scenario**: If you already have a high-performing individual model that gives satisfactory results, adding ensemble methods might not provide substantial improvement.\n",
        "   - **Reason to Avoid**: Ensemble methods may only offer marginal improvements over well-performing single models, especially in cases where the base model is already highly optimized.\n",
        "\n",
        "### Summary:\n",
        "Avoid ensemble methods in situations where:\n",
        "- The problem is simple, and a single model suffices.\n",
        "- Computational resources (memory, time) are limited.\n",
        "- Interpretability is crucial.\n",
        "- You have small or noisy data.\n",
        "- Speed and simplicity are prioritized (e.g., real-time systems).\n",
        "- Overfitting is a significant concern.\n",
        "- You already have a high-performing model.\n",
        "\n",
        "In these cases, simpler models might be more appropriate and more efficient than complex ensemble techniques."
      ],
      "metadata": {
        "id": "5EfrGl4rUgV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.16 How does Bagging help in reducing overfitting?"
      ],
      "metadata": {
        "id": "CYcznPhvU0M2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.16 Bagging (Bootstrap Aggregating) helps reduce overfitting by combining the predictions of multiple models trained on different subsets of the training data. Here's how it works and how it helps reduce overfitting:\n",
        "\n",
        "1. **Bootstrap Sampling**: In bagging, multiple subsets of the data are created by randomly sampling with replacement from the training dataset (this is called bootstrap sampling). Each subset is used to train a separate model, typically a weak learner like a decision tree.\n",
        "\n",
        "2. **Ensemble of Models**: Instead of using a single model, bagging builds an ensemble of models. Each model in the ensemble is trained on a slightly different dataset, which introduces diversity among the models.\n",
        "\n",
        "3. **Averaging or Majority Voting**: After training, the predictions from all the models are combined. For regression tasks, predictions are typically averaged, while for classification tasks, majority voting is used. This means the final prediction depends on the collective wisdom of the models rather than one model alone.\n",
        "\n",
        "### How Bagging Reduces Overfitting:\n",
        "- **Reduces Variance**: Bagging reduces the variance of predictions. When models are trained on different subsets of the data, each model may make different errors, and averaging or voting helps smooth out these errors. By reducing the variance in predictions, bagging helps prevent the model from overfitting to noise or outliers in the training data.\n",
        "  \n",
        "- **Avoids Overfitting of Individual Models**: Weak models like decision trees are prone to overfitting, especially when they are deep. Bagging helps to reduce the tendency of individual models to overfit by combining them. Since the predictions from multiple models are averaged or voted on, the risk of overfitting to any specific subset of the data is diminished.\n",
        "\n",
        "- **Bias-Variance Tradeoff**: While bagging may not reduce the bias of individual models (e.g., decision trees still may be biased if they're not fully grown), it reduces variance significantly. By using multiple models, bagging decreases the overall model's variance, leading to better generalization to unseen data.\n",
        "\n",
        "In essence, bagging helps by creating an ensemble of models that are more robust to overfitting, as the individual weaknesses of each model are offset by the collective strength of the group."
      ],
      "metadata": {
        "id": "1EK9FHSlVAL5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.17 Why is Random Forest better than a single Decision Tree?"
      ],
      "metadata": {
        "id": "kxbRfK6oVbpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.17 Random Forest is generally better than a single decision tree because it addresses some of the limitations and weaknesses of decision trees by leveraging multiple trees in an ensemble. Here’s a breakdown of why Random Forest tends to outperform a single Decision Tree:\n",
        "\n",
        "### 1. **Reduction in Overfitting (Generalization)**\n",
        "- **Decision Tree**: A single decision tree is prone to overfitting, especially when it is deep. It can capture noise or small fluctuations in the data, leading to poor generalization on new, unseen data. The tree essentially memorizes the training data.\n",
        "  \n",
        "- **Random Forest**: By averaging the predictions of multiple decision trees, each trained on a different subset of the data (using bootstrap sampling), Random Forest reduces the overfitting problem. The diversity among the individual trees helps correct errors made by any single tree, leading to better generalization and reducing the variance.\n",
        "\n",
        "### 2. **Better Accuracy**\n",
        "- **Decision Tree**: A single decision tree often makes strong decisions based on the training set, but those decisions can be overly specific to the training data. It’s highly sensitive to the noise or outliers in the dataset, which can degrade its performance.\n",
        "  \n",
        "- **Random Forest**: By combining multiple decision trees, Random Forest averages out the biases of individual trees and uses the majority vote (for classification) or averaging (for regression). This generally leads to improved predictive accuracy and robustness over a single tree.\n",
        "\n",
        "### 3. **Handling of Feature Correlation**\n",
        "- **Decision Tree**: In a single decision tree, all decisions are made based on the entire dataset, meaning the tree can sometimes rely too heavily on correlated features, leading to suboptimal splits.\n",
        "  \n",
        "- **Random Forest**: Random Forest uses a technique called **feature bagging**. In each tree, only a random subset of features is considered for splitting at each node. This encourages diversity among the trees and prevents overfitting to any particular set of features, improving model robustness.\n",
        "\n",
        "### 4. **Robustness to Outliers and Noise**\n",
        "- **Decision Tree**: A single decision tree can be very sensitive to outliers or noise in the data. If there are a few data points that are significantly different from the rest, the tree might end up making biased splits, leading to poor performance.\n",
        "  \n",
        "- **Random Forest**: By averaging the predictions of many trees, the influence of any single outlier is minimized. Random Forest is inherently more robust to noise and outliers because the collective decision-making process of the trees helps smooth out any inconsistencies caused by noisy data points.\n",
        "\n",
        "### 5. **Improved Stability**\n",
        "- **Decision Tree**: A small change in the training data can result in a drastically different decision tree. This lack of stability means that the model may not consistently make accurate predictions when faced with new data.\n",
        "  \n",
        "- **Random Forest**: Because Random Forest uses an ensemble of trees, it is much more stable. Even if a few trees in the forest are inaccurate due to some fluctuations in the data, the overall model still performs well due to the majority voting mechanism (or averaging in regression tasks).\n",
        "\n",
        "### 6. **Feature Importance Evaluation**\n",
        "- **Decision Tree**: A single decision tree can give you feature importance, but the importance might be skewed depending on the tree's structure and the features it happens to choose.\n",
        "  \n",
        "- **Random Forest**: Random Forest can give more reliable estimates of feature importance by averaging across all the trees in the forest. This makes it more robust and less sensitive to any individual tree's bias toward certain features.\n",
        "\n",
        "### 7. **Bias-Variance Tradeoff**\n",
        "- **Decision Tree**: A single decision tree typically has high variance, meaning it’s very sensitive to changes in the training data and could overfit, leading to poor generalization.\n",
        "  \n",
        "- **Random Forest**: Random Forest reduces the variance by averaging the predictions of many trees. While it doesn’t necessarily reduce bias significantly (since each tree in the forest is still a decision tree), it strikes a much better balance between bias and variance, leading to superior performance.\n",
        "\n",
        "### 8. **Parallelization**\n",
        "- **Decision Tree**: Training a single decision tree is not easily parallelizable because it’s a sequential process where each split depends on the previous one.\n",
        "  \n",
        "- **Random Forest**: Each tree in a Random Forest can be trained independently, which makes the model highly parallelizable. This allows for faster training on large datasets.\n",
        "\n",
        "### Summary:\n",
        "- **Single Decision Tree**: Simple, interpretable, but prone to overfitting and poor generalization.\n",
        "- **Random Forest**: An ensemble of decision trees that improves generalization by reducing variance, increasing accuracy, handling noise and outliers better, and providing more reliable feature importance.\n",
        "\n",
        "Overall, **Random Forest** combines the strengths of multiple decision trees while mitigating the weaknesses of any single tree, making it a more powerful and reliable model in practice."
      ],
      "metadata": {
        "id": "zV3P0um5Vnzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.18 What is the role of bootstrap sampling in Bagging?"
      ],
      "metadata": {
        "id": "TCllV9bxV6cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.18 The role of **bootstrap sampling** in **Bagging (Bootstrap Aggregating)** is crucial for creating diverse training subsets from the original dataset, which helps in reducing the model's variance and improving its generalization ability. Here’s how it works and why it's important:\n",
        "\n",
        "### 1. **Creating Multiple Subsets of Data (Sampling with Replacement)**:\n",
        "- In **bootstrap sampling**, multiple new training sets are created by randomly sampling data points **with replacement** from the original dataset.\n",
        "- Each new training set is the same size as the original dataset, but it may contain duplicate data points while other data points might be left out.\n",
        "  \n",
        "  **Example**: If the original dataset has 100 samples, a new bootstrap sample might also have 100 samples, but some samples from the original dataset will appear multiple times, while others might be missing.\n",
        "\n",
        "### 2. **Diversity Among Models**:\n",
        "- Since each bootstrap sample is different (due to random sampling with replacement), each model (usually a decision tree) is trained on a different subset of data.\n",
        "- This **diversity** among the models is key to improving the overall performance of the ensemble. While individual models might overfit or be biased towards certain data points, the averaging (or majority voting) process in Bagging helps reduce these effects.\n",
        "\n",
        "### 3. **Reducing Overfitting**:\n",
        "- **Overfitting** occurs when a model learns the noise or random fluctuations in the training data, making it poorly generalize to new data.\n",
        "- By training multiple models on different bootstrap samples, Bagging helps **reduce overfitting**. Even if some individual models overfit the noise in their bootstrap sample, the ensemble of models is less likely to overfit as a whole because errors from individual models tend to cancel each other out when combined.\n",
        "\n",
        "### 4. **Improving Generalization**:\n",
        "- Bagging increases the **stability** and **generalization** of the final model by reducing variance.\n",
        "- When predictions from multiple models (trained on different bootstrap samples) are aggregated, the resulting prediction is more robust and generalizes better to unseen data.\n",
        "\n",
        "### 5. **Handling Outliers and Noise**:\n",
        "- Since some data points are not included in each bootstrap sample (about one-third of the data points are excluded on average), each model has a slightly different perspective on the dataset.\n",
        "- This helps in reducing the influence of outliers or noisy data points that might otherwise dominate a single model’s predictions.\n",
        "\n",
        "### 6. **Improving Accuracy**:\n",
        "- By averaging the predictions of several models, Bagging often leads to a **lower variance** compared to a single model, which results in better predictive performance and more accurate results.\n",
        "\n",
        "### In summary:\n",
        "- **Bootstrap sampling** in Bagging creates multiple diverse training sets from the original data by sampling with replacement. This diversity helps in reducing variance, improving generalization, and mitigating the risk of overfitting, resulting in a more accurate and stable model."
      ],
      "metadata": {
        "id": "sSHLp9upWT6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.19 What are some real-world applications of ensemble techniques?"
      ],
      "metadata": {
        "id": "x8OW5Ut0Wmkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.19 Ensemble techniques are widely used across various industries and applications because they generally improve the performance and robustness of machine learning models. Here are some **real-world applications** of ensemble techniques like **Bagging**, **Boosting**, and **Stacking**:\n",
        "\n",
        "### 1. **Finance and Credit Scoring**\n",
        "- **Application**: Predicting creditworthiness and fraud detection.\n",
        "- **Ensemble Method**: **Random Forest**, **Gradient Boosting**.\n",
        "- **Why**: In finance, predicting whether a customer will default on a loan or identifying fraudulent transactions involves dealing with highly complex, non-linear data. Ensemble techniques improve model accuracy by aggregating multiple weak learners (like decision trees), which helps reduce the risk of misclassifications in a dynamic environment.\n",
        "\n",
        "### 2. **Healthcare and Medical Diagnosis**\n",
        "- **Application**: Disease prediction, diagnosis, and medical image analysis.\n",
        "- **Ensemble Method**: **Random Forest**, **XGBoost**, **AdaBoost**.\n",
        "- **Why**: In healthcare, predicting patient outcomes, diagnosing diseases from medical images, or analyzing electronic health records involves diverse data sources with noise and complexity. Ensemble methods can combine different models to provide more reliable and accurate predictions, thus aiding in early diagnosis and decision-making.\n",
        "\n",
        "### 3. **Marketing and Customer Segmentation**\n",
        "- **Application**: Customer churn prediction, targeted marketing campaigns, and recommendation systems.\n",
        "- **Ensemble Method**: **Random Forest**, **Gradient Boosting Machines (GBM)**.\n",
        "- **Why**: Marketing and customer analysis often involve datasets with numerous features and varying importance. Ensemble techniques help by aggregating multiple models to capture a broader range of customer behaviors and interactions, improving the ability to predict churn, recommend products, or segment customers accurately.\n",
        "\n",
        "### 4. **E-Commerce and Recommendation Systems**\n",
        "- **Application**: Product recommendation and personalized search.\n",
        "- **Ensemble Method**: **Stacking**, **Random Forest**, **Gradient Boosting**.\n",
        "- **Why**: Online retailers and streaming services use recommendation systems to suggest products, movies, or content to users. Ensemble methods combine predictions from multiple models (e.g., collaborative filtering, content-based models) to improve the relevance of recommendations by capturing different aspects of user behavior and preferences.\n",
        "\n",
        "### 5. **Autonomous Vehicles**\n",
        "- **Application**: Object detection, navigation, and decision-making.\n",
        "- **Ensemble Method**: **Random Forest**, **Boosting (AdaBoost, XGBoost)**.\n",
        "- **Why**: Autonomous driving involves multiple sensors and data types (e.g., cameras, LiDAR, radar) that need to be integrated to make real-time decisions. Ensemble models can combine the results of different algorithms (e.g., object detection, path planning, and decision-making) to improve accuracy, robustness, and safety.\n",
        "\n",
        "### 6. **Spam Detection and Text Classification**\n",
        "- **Application**: Email spam filtering, sentiment analysis, and content moderation.\n",
        "- **Ensemble Method**: **Random Forest**, **Gradient Boosting**, **Voting Classifier**.\n",
        "- **Why**: Text classification tasks, like filtering spam emails or sentiment analysis, involve high-dimensional data with noisy or ambiguous patterns. Ensemble techniques help by combining multiple classifiers to reduce bias and variance, leading to more accurate predictions in these tasks.\n",
        "\n",
        "### 7. **Fraud Detection and Security**\n",
        "- **Application**: Financial fraud detection, cybersecurity, and anomaly detection.\n",
        "- **Ensemble Method**: **Random Forest**, **XGBoost**, **AdaBoost**.\n",
        "- **Why**: Fraud detection systems in banking, e-commerce, and cybersecurity require highly accurate models that can detect subtle anomalies. Ensemble techniques combine the predictions of several models, reducing false positives and improving the model's ability to detect rare fraud events that are hard to capture with single models.\n",
        "\n",
        "### 8. **Image Classification and Computer Vision**\n",
        "- **Application**: Face recognition, object detection, and medical image segmentation.\n",
        "- **Ensemble Method**: **Random Forest**, **Gradient Boosting**, **Voting Classifier**, **Stacking**.\n",
        "- **Why**: In computer vision, complex patterns need to be detected, and individual models may struggle to capture all the nuances in the data. Ensemble methods combine predictions from different models, which improves accuracy and robustness when recognizing objects, detecting anomalies, or analyzing medical images like MRIs and X-rays.\n",
        "\n",
        "### 9. **Speech Recognition and Natural Language Processing (NLP)**\n",
        "- **Application**: Voice assistants, language translation, and sentiment analysis.\n",
        "- **Ensemble Method**: **Stacking**, **Voting Classifier**, **Random Forest**.\n",
        "- **Why**: NLP tasks, such as translating languages or classifying text sentiment, involve highly varied and often noisy input data. Ensemble methods help by combining models trained on different aspects of the language (e.g., n-grams, deep learning models, or traditional machine learning classifiers), improving the overall performance of speech recognition or sentiment prediction tasks.\n",
        "\n",
        "### 10. **Weather Forecasting and Climate Modeling**\n",
        "- **Application**: Predicting weather conditions, natural disasters, and climate changes.\n",
        "- **Ensemble Method**: **Bagging**, **Boosting** (e.g., XGBoost), **Random Forest**.\n",
        "- **Why**: Weather prediction models need to account for highly complex and chaotic data, like temperature, humidity, and pressure. Ensemble techniques help by combining predictions from multiple models to reduce errors and improve the accuracy and reliability of forecasts, such as predicting rainfall or storm patterns.\n",
        "\n",
        "### 11. **Manufacturing and Quality Control**\n",
        "- **Application**: Predicting machine failures, defect detection, and optimizing production lines.\n",
        "- **Ensemble Method**: **Random Forest**, **Gradient Boosting Machines (GBM)**.\n",
        "- **Why**: In manufacturing, accurate predictive maintenance models are critical for preventing costly machine downtime. Ensemble methods can combine the outputs of multiple models trained on sensor data, enabling more accurate detection of faulty equipment or defects in production lines.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary:\n",
        "Ensemble techniques are used across a wide range of industries to improve the accuracy, robustness, and generalization of machine learning models. Whether it’s for credit scoring, fraud detection, recommendation systems, or image classification, ensemble methods help address challenges such as high variance, overfitting, and noisy data, making them invaluable tools in real-world applications."
      ],
      "metadata": {
        "id": "GFm-iM1PWz8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.20 What is the difference between Bagging and Boosting?"
      ],
      "metadata": {
        "id": "8n03n8-TXL1F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.20 **Bagging** and **Boosting** are both ensemble learning techniques that aim to improve the performance of machine learning models by combining multiple models, but they do so in different ways. Here's a detailed comparison of the two:\n",
        "\n",
        "### 1. **Method of Combining Models**:\n",
        "- **Bagging (Bootstrap Aggregating)**:\n",
        "  - **Parallel**: In bagging, multiple models (usually weak learners, such as decision trees) are trained **in parallel**.\n",
        "  - **Voting/Averaging**: After training, the predictions of all the models are combined. For classification, majority voting is used, and for regression, the predictions are averaged.\n",
        "  \n",
        "- **Boosting**:\n",
        "  - **Sequential**: In boosting, models are trained **sequentially**, where each new model is trained to correct the mistakes made by the previous ones.\n",
        "  - **Weighted Aggregation**: The final prediction is a weighted average or sum of the predictions of all models, with models that perform better being given more weight in the final decision.\n",
        "\n",
        "### 2. **Focus on Errors**:\n",
        "- **Bagging**:\n",
        "  - Bagging trains each model independently on a random subset of the data (using bootstrap sampling), and the focus is **not specifically on correcting errors** from previous models. Every model in the ensemble is treated equally when making the final prediction.\n",
        "  \n",
        "- **Boosting**:\n",
        "  - Boosting focuses on **correcting the errors made by previous models**. Each subsequent model is trained to give more importance (weight) to the data points that were misclassified by previous models. It **emphasizes harder-to-classify instances** to improve the model’s performance.\n",
        "\n",
        "### 3. **Bias-Variance Tradeoff**:\n",
        "- **Bagging**:\n",
        "  - Bagging primarily reduces **variance** by averaging or voting over multiple models. It is most beneficial when the base model has high variance (e.g., decision trees), as it helps smooth out predictions and reduces overfitting.\n",
        "  \n",
        "- **Boosting**:\n",
        "  - Boosting primarily reduces **bias** by focusing on difficult instances and improving the accuracy of weak learners. While it can reduce both bias and variance, it can be prone to **overfitting** if the number of iterations is too large or if the base model is too complex.\n",
        "\n",
        "### 4. **Model Training**:\n",
        "- **Bagging**:\n",
        "  - Each model is trained **independently** on different random subsets of the data.\n",
        "  - Random Forest, a popular algorithm, is a type of bagging technique where multiple decision trees are built in parallel.\n",
        "\n",
        "- **Boosting**:\n",
        "  - Models are trained **sequentially**, with each model focusing on the errors made by the previous one.\n",
        "  - Popular boosting algorithms include **AdaBoost**, **Gradient Boosting**, and **XGBoost**.\n",
        "\n",
        "### 5. **Handling of Data**:\n",
        "- **Bagging**:\n",
        "  - **Random sampling with replacement** (bootstrap sampling) creates multiple subsets of the data, and each model is trained on a different subset.\n",
        "  - **Out-of-Bag (OOB) data**: In bagging, about one-third of the data is not used for training a particular model, and it can be used to evaluate the model's performance, which is a form of cross-validation.\n",
        "\n",
        "- **Boosting**:\n",
        "  - Boosting does not use random sampling. Each model is trained on the **entire dataset** (with adjusted weights based on the previous model’s performance). The misclassified instances from the previous model get higher weights, making them more important for the next model.\n",
        "\n",
        "### 6. **Parallelism**:\n",
        "- **Bagging**:\n",
        "  - Since all models are trained independently, bagging can be **parallelized** easily. Each model can be trained at the same time, making it more efficient when using parallel computation resources.\n",
        "\n",
        "- **Boosting**:\n",
        "  - Boosting involves sequential model training, meaning it **cannot be parallelized** as easily because each model depends on the results of the previous one. However, some variations (like XGBoost) have implemented parallelization at certain stages.\n",
        "\n",
        "### 7. **Performance**:\n",
        "- **Bagging**:\n",
        "  - Bagging generally works best when the base models are **high-variance, low-bias** models (e.g., decision trees), as it reduces the variance without significantly increasing the bias. It is robust against overfitting.\n",
        "  \n",
        "- **Boosting**:\n",
        "  - Boosting is effective when the base models are **weak learners**, and it can significantly improve both bias and variance, but it is more sensitive to **overfitting** compared to bagging.\n",
        "\n",
        "### 8. **Robustness to Outliers**:\n",
        "- **Bagging**:\n",
        "  - Since models are trained on different subsets of data, and each model is independent, bagging is more **robust to outliers**. The outliers may appear in only some of the models and have less influence on the final prediction.\n",
        "  \n",
        "- **Boosting**:\n",
        "  - Boosting is **sensitive to outliers** because it gives more importance to misclassified instances. If an outlier is misclassified, it can be repeatedly weighted heavily, potentially distorting the model’s overall performance.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Differences Summary:\n",
        "\n",
        "| Feature                | **Bagging**                                 | **Boosting**                                |\n",
        "|------------------------|---------------------------------------------|---------------------------------------------|\n",
        "| **Model Training**      | Parallel (independent models)               | Sequential (each model depends on the last) |\n",
        "| **Focus**               | Reduce variance                            | Reduce bias and improve accuracy            |\n",
        "| **Error Handling**      | No focus on errors, models trained independently | Focuses on correcting previous model errors |\n",
        "| **Bias/Variance**       | Reduces variance                           | Reduces bias, but can increase variance     |\n",
        "| **Parallelism**         | Can be parallelized                        | Cannot be easily parallelized               |\n",
        "| **Robustness**          | More robust to overfitting and outliers    | Prone to overfitting, especially with noisy data |\n",
        "| **Examples**            | Random Forest, Bagging Classifier          | AdaBoost, Gradient Boosting, XGBoost       |\n",
        "\n",
        "---\n",
        "\n",
        "### In Summary:\n",
        "- **Bagging** is useful when you want to **reduce variance** and improve the stability of high-variance models (like decision trees).\n",
        "- **Boosting** is ideal for **improving accuracy** and **reducing bias**, but it requires more careful tuning to avoid overfitting. Boosting works best when trying to improve weak models by focusing on their mistakes."
      ],
      "metadata": {
        "id": "OUsTjOaQXV2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Questions>"
      ],
      "metadata": {
        "id": "mVGEaV7YX6Si"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.21 Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy."
      ],
      "metadata": {
        "id": "AJAw7_fNY4bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier as the base estimator\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with the Decision Tree Classifier as the base estimator\n",
        "bagging_classifier = BaggingClassifier(estimator=dt_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Bagging Classifier: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpKVztbmFlq1",
        "outputId": "b605cda4-6055-49cf-be54-ad093e8f119f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Bagging Classifier: 0.885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.22 2 Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "2krf4HhmGItr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor as the base estimator\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Create a Bagging Regressor with the Decision Tree Regressor as the base estimator\n",
        "bagging_regressor = BaggingRegressor(estimator=dt_regressor, n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the Bagging Regressor\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE) of Bagging Regressor: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFSvcqqUGxjs",
        "outputId": "cea6c0b0-8838-406d-9b06-21be9fef31d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) of Bagging Regressor: 7484.147276569565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.23 Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores."
      ],
      "metadata": {
        "id": "eWhuAlrlHMXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Print feature importance scores\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"Feature {data.feature_names[i]}: {importance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNLBUU4EHdfX",
        "outputId": "a01bed9d-6919-4e90-b2ad-8599a5135670"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature mean radius: 0.048703371737755234\n",
            "Feature mean texture: 0.013590877656998469\n",
            "Feature mean perimeter: 0.053269746128179675\n",
            "Feature mean area: 0.04755500886018552\n",
            "Feature mean smoothness: 0.007285327830663239\n",
            "Feature mean compactness: 0.013944325074050485\n",
            "Feature mean concavity: 0.06800084191430111\n",
            "Feature mean concave points: 0.10620998844591638\n",
            "Feature mean symmetry: 0.003770291819290666\n",
            "Feature mean fractal dimension: 0.0038857721093275\n",
            "Feature radius error: 0.02013891719419153\n",
            "Feature texture error: 0.004723988073894702\n",
            "Feature perimeter error: 0.01130301388178435\n",
            "Feature area error: 0.022406960160458473\n",
            "Feature smoothness error: 0.004270910110504497\n",
            "Feature compactness error: 0.005253215538990106\n",
            "Feature concavity error: 0.009385832251596627\n",
            "Feature concave points error: 0.003513255105598506\n",
            "Feature symmetry error: 0.004018418617722808\n",
            "Feature fractal dimension error: 0.00532145634222884\n",
            "Feature worst radius: 0.07798687515738047\n",
            "Feature worst texture: 0.021749011006763207\n",
            "Feature worst perimeter: 0.06711483267839194\n",
            "Feature worst area: 0.15389236463205394\n",
            "Feature worst smoothness: 0.010644205147280952\n",
            "Feature worst compactness: 0.020266035899623565\n",
            "Feature worst concavity: 0.031801595740040434\n",
            "Feature worst concave points: 0.14466326620735528\n",
            "Feature worst symmetry: 0.010120176131974357\n",
            "Feature worst fractal dimension: 0.005210118545497296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.24 Train a Random Forest Regressor and compare its performance with a single Decision Tree."
      ],
      "metadata": {
        "id": "6JrtEZaeHiP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "dt_y_pred = dt_regressor.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_y_pred)\n",
        "print(f\"Mean Squared Error (MSE) of Decision Tree Regressor: {dt_mse}\")\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "rf_y_pred = rf_regressor.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
        "print(f\"Mean Squared Error (MSE) of Random Forest Regressor: {rf_mse}\")\n",
        "\n",
        "# Compare performance\n",
        "print(f\"Decision Tree MSE: {dt_mse}\")\n",
        "print(f\"Random Forest MSE: {rf_mse}\")\n",
        "if dt_mse < rf_mse:\n",
        "    print(\"Decision Tree performed better.\")\n",
        "else:\n",
        "    print(\"Random Forest performed better.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwZ4WFFWH5Jn",
        "outputId": "6504d8e5-e159-4261-d163-1d9b2abb0353"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) of Decision Tree Regressor: 20519.297540712625\n",
            "Mean Squared Error (MSE) of Random Forest Regressor: 7055.507694741972\n",
            "Decision Tree MSE: 20519.297540712625\n",
            "Random Forest MSE: 7055.507694741972\n",
            "Random Forest performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.25 Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier."
      ],
      "metadata": {
        "id": "uJy0GTguIC5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier with OOB scoring enabled\n",
        "rf_classifier = RandomForestClassifier(random_state=42, oob_score=True, n_estimators=100)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get the OOB score\n",
        "oob_score = rf_classifier.oob_score_\n",
        "\n",
        "# Print the OOB score\n",
        "print(f\"Out-of-Bag (OOB) Score: {oob_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEBmV8IwITC1",
        "outputId": "e4d976d7-a9b0-4c2f-9d3a-3716e89174ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-Bag (OOB) Score: 0.9560439560439561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.26 Train a Bagging Classifier using SVM as a base estimator and print accuracy."
      ],
      "metadata": {
        "id": "PLjbIEW5IaeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an SVM classifier as the base estimator\n",
        "svm_classifier = SVC(random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with the SVM classifier as the base estimator\n",
        "bagging_classifier = BaggingClassifier(estimator=svm_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Bagging Classifier with SVM: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1c_MY_eIvlI",
        "outputId": "81c76444-dc63-4aa7-8293-3430e020278b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Bagging Classifier with SVM: 0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.27 Train a Random Forest Classifier with different numbers of trees and compare accuracy."
      ],
      "metadata": {
        "id": "INJ_YoDmI0bn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Number of trees to try\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    # Create a Random Forest Classifier\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
        "\n",
        "    # Train the Random Forest Classifier\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with {n_estimators} trees: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KyO2KjQJDOg",
        "outputId": "85aa9d43-403a-4183-afa5-acfca0e0aa9a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 10 trees: 0.956140350877193\n",
            "Accuracy with 50 trees: 0.9649122807017544\n",
            "Accuracy with 100 trees: 0.9649122807017544\n",
            "Accuracy with 200 trees: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.28 Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score."
      ],
      "metadata": {
        "id": "ehkp2DXGJIss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression classifier as the base estimator\n",
        "lr_classifier = LogisticRegression(random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier with the Logistic Regression classifier as the base estimator\n",
        "bagging_classifier = BaggingClassifier(estimator=lr_classifier, n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_proba = bagging_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate and print the AUC score\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"AUC score of Bagging Classifier with Logistic Regression: {auc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apRNOdtRJg2K",
        "outputId": "d6e07405-ab83-4f0e-919a-e7ad8f0632ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC score of Bagging Classifier with Logistic Regression: 0.9225203497135966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.29 Train a Random Forest Regressor and analyze feature importance scores."
      ],
      "metadata": {
        "id": "53FaRTfsJk0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Train the Random Forest Regressor\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_regressor.feature_importances_\n",
        "\n",
        "# Print feature importance scores\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"Feature {i}: {importance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbzUtsnybyYt",
        "outputId": "49db427a-235f-49f1-9521-870e9ea849be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature 0: 0.14680305972208604\n",
            "Feature 1: 0.23690793203024726\n",
            "Feature 2: 0.009577318842801054\n",
            "Feature 3: 0.008526566009500549\n",
            "Feature 4: 0.17218186599638538\n",
            "Feature 5: 0.007724789280131386\n",
            "Feature 6: 0.10510844030156447\n",
            "Feature 7: 0.008972526889733259\n",
            "Feature 8: 0.008674606418996285\n",
            "Feature 9: 0.007482427240831205\n",
            "Feature 10: 0.009062529081082004\n",
            "Feature 11: 0.025764742032778613\n",
            "Feature 12: 0.00788843591514188\n",
            "Feature 13: 0.00784145693941909\n",
            "Feature 14: 0.00836636506560795\n",
            "Feature 15: 0.018097455953040107\n",
            "Feature 16: 0.008750511586082923\n",
            "Feature 17: 0.1848067087704713\n",
            "Feature 18: 0.009548703611637248\n",
            "Feature 19: 0.007913558312462073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.30 Train an ensemble model using both Bagging and Random Forest and compare accuracy."
      ],
      "metadata": {
        "id": "logpBtXgKVqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging Classifier\n",
        "bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "bagging_predictions = bagging_classifier.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
        "print(f\"Bagging Classifier Accuracy: {bagging_accuracy}\")\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "rf_predictions = rf_classifier.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "print(f\"Random Forest Classifier Accuracy: {rf_accuracy}\")\n",
        "\n",
        "# Compare Accuracy\n",
        "if bagging_accuracy > rf_accuracy:\n",
        "  print(\"Bagging Classifier performs better.\")\n",
        "elif rf_accuracy > bagging_accuracy:\n",
        "  print(\"Random Forest Classifier performs better.\")\n",
        "else:\n",
        "  print(\"Both models perform equally.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORz630tbKloD",
        "outputId": "50582b00-b3e5-4116-85ff-26655a893843"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.885\n",
            "Random Forest Classifier Accuracy: 0.855\n",
            "Bagging Classifier performs better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.31 Train a Random Forest Classifier and tune hyperparameters using GridSearchCV"
      ],
      "metadata": {
        "id": "sdENzD-dKuRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV object to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_rf_classifier = grid_search.best_estimator_\n",
        "y_pred = best_rf_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of best model: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Icn0x11VLf3A",
        "outputId": "28203fb8-8fbb-424b-bdc9-06e544052e29"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Accuracy of best model: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.32 Train a Bagging Regressor with different numbers of base estimators and compare performance."
      ],
      "metadata": {
        "id": "bD9TtI4nL12c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Number of base estimators to try\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    # Create a Bagging Regressor\n",
        "    bagging_regressor = BaggingRegressor(n_estimators=n_estimators, random_state=42)\n",
        "\n",
        "    # Train the Bagging Regressor\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "    # Evaluate MSE\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"MSE with {n_estimators} estimators: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EciHt9fMH4k",
        "outputId": "35d9b74b-3f0b-4da5-b749-5bd0f8c4e394"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE with 10 estimators: 7484.147276569565\n",
            "MSE with 50 estimators: 7109.3878911872125\n",
            "MSE with 100 estimators: 6967.863746774799\n",
            "MSE with 200 estimators: 6688.668787904271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.33 Train a Random Forest Classifier and analyze misclassified samples."
      ],
      "metadata": {
        "id": "b9BUW2xZMOai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Find misclassified samples\n",
        "misclassified_indices = y_pred != y_test\n",
        "\n",
        "# Analyze misclassified samples (example: print indices and true/predicted labels)\n",
        "print(\"Misclassified Samples:\")\n",
        "for i in range(len(y_test)):\n",
        "  if misclassified_indices[i]:\n",
        "    print(f\"Index: {i}, True Label: {y_test[i]}, Predicted Label: {y_pred[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riYzTGZhMc06",
        "outputId": "0ff4c560-e29c-4880-8500-858c9403e235"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misclassified Samples:\n",
            "Index: 8, True Label: 1, Predicted Label: 0\n",
            "Index: 20, True Label: 0, Predicted Label: 1\n",
            "Index: 77, True Label: 0, Predicted Label: 1\n",
            "Index: 82, True Label: 0, Predicted Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.34 Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier."
      ],
      "metadata": {
        "id": "n5aUyp1EMhEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined from previous code\n",
        "\n",
        "# Train a single Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "dt_y_pred = dt_classifier.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_y_pred)\n",
        "print(f\"Accuracy of Decision Tree Classifier: {dt_accuracy}\")\n",
        "\n",
        "# Assuming bagging_classifier is already trained from previous code\n",
        "\n",
        "# Compare performance\n",
        "print(f\"Decision Tree Accuracy: {dt_accuracy}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy}\") # accuracy from previous code block\n",
        "if dt_accuracy > accuracy:\n",
        "    print(\"Decision Tree performed better.\")\n",
        "elif accuracy > dt_accuracy :\n",
        "    print(\"Bagging Classifier performed better.\")\n",
        "else:\n",
        "    print(\"Both models performed equally.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ndbS3ROMzPr",
        "outputId": "3a91fa12-5ae3-4278-eadc-a1ae3f7d6618"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree Classifier: 0.9473684210526315\n",
            "Decision Tree Accuracy: 0.9473684210526315\n",
            "Bagging Classifier Accuracy: 0.9649122807017544\n",
            "Bagging Classifier performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.35 Train a Random Forest Classifier and visualize the confusion matrix."
      ],
      "metadata": {
        "id": "csPFubjxM4LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming X_test, y_test, and rf_classifier are defined from previous code\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Create the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "94zt1bLFNImx",
        "outputId": "a48ba4bc-b536-4ed7-cc65-5afc43b57674"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.36 Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy"
      ],
      "metadata": {
        "id": "apoDwbVpNNaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the base estimators\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),  # probability=True is required for stacking\n",
        "    ('lr', LogisticRegression(random_state=42))\n",
        "]\n",
        "\n",
        "# Define the meta-learner\n",
        "meta_learner = LogisticRegression(random_state=42)\n",
        "\n",
        "# Create the stacking classifier\n",
        "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=meta_learner, cv=5)\n",
        "\n",
        "\n",
        "# Train the stacking classifier\n",
        "stacking_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_stacking = stacking_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the stacking classifier\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "print(f\"Accuracy of Stacking Classifier: {accuracy_stacking}\")\n",
        "\n",
        "# Train individual classifiers for comparison\n",
        "for name, estimator in estimators:\n",
        "    estimator.fit(X_train, y_train)\n",
        "    y_pred = estimator.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test,y_pred)\n",
        "    print(f\"Accuracy of {name}: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSaMOLEzNcvo",
        "outputId": "2ba26b43-6519-4445-cc05-653527e5ac9f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Stacking Classifier: 0.865\n",
            "Accuracy of dt: 0.875\n",
            "Accuracy of svm: 0.845\n",
            "Accuracy of lr: 0.855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.37 Train a Random Forest Classifier and print the top 5 most important features."
      ],
      "metadata": {
        "id": "kUATGDSZNi8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'rf_classifier' and 'data' are defined from the previous code block.\n",
        "# If not, load the data and train the classifier as shown in QUE.23\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Get the indices of the top 5 most important features\n",
        "top_5_indices = np.argsort(feature_importances)[::-1][:5]\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 most important features:\")\n",
        "for i in top_5_indices:\n",
        "    print(f\"{data.feature_names[i]}: {feature_importances[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK7ibV2ONzYy",
        "outputId": "3ab2fd87-ad48-4d7e-902f-125d7a2b1b35"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most important features:\n",
            "worst area: 0.15389236463205394\n",
            "worst concave points: 0.14466326620735528\n",
            "mean concave points: 0.10620998844591638\n",
            "worst radius: 0.07798687515738047\n",
            "mean concavity: 0.06800084191430111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.38 Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score."
      ],
      "metadata": {
        "id": "ILXJxonJN6V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier\n",
        "bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioq3Js5bOL8a",
        "outputId": "5c3e43d1-aed3-49e9-a1ed-1592bde3ff16"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9565217391304348\n",
            "Recall: 0.822429906542056\n",
            "F1-score: 0.8844221105527639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.39 Train a Random Forest Classifier and analyze the effect of max_depth on accuracy."
      ],
      "metadata": {
        "id": "kDjKBdKlOQRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a range of max_depth values to test\n",
        "max_depth_values = [None, 5, 10, 15, 20]\n",
        "accuracy_scores = []\n",
        "\n",
        "for max_depth in max_depth_values:\n",
        "    # Create a Random Forest Classifier with the current max_depth\n",
        "    rf_classifier = RandomForestClassifier(max_depth=max_depth, random_state=42)\n",
        "\n",
        "    # Train the Random Forest Classifier\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "    print(f\"Max Depth: {max_depth}, Accuracy: {accuracy}\")\n",
        "\n",
        "# Analyze the results (e.g., plot accuracy vs. max_depth)\n",
        "# You can use matplotlib or other plotting libraries to visualize the relationship\n",
        "# between max_depth and accuracy.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItY4-B4LOg_6",
        "outputId": "adcf946f-70a7-40ce-c2cf-da80009ec7dc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Depth: None, Accuracy: 0.9649122807017544\n",
            "Max Depth: 5, Accuracy: 0.9649122807017544\n",
            "Max Depth: 10, Accuracy: 0.9649122807017544\n",
            "Max Depth: 15, Accuracy: 0.9649122807017544\n",
            "Max Depth: 20, Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.40 Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare Performance."
      ],
      "metadata": {
        "id": "zml2er37Oozb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging Regressor with Decision Tree as the base estimator\n",
        "bagging_dt = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "y_pred_dt = bagging_dt.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "print(f\"Bagging with Decision Tree MSE: {mse_dt}\")\n",
        "\n",
        "# Bagging Regressor with K-Nearest Neighbors as the base estimator\n",
        "bagging_knn = BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=10, random_state=42)\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "y_pred_knn = bagging_knn.predict(X_test)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "print(f\"Bagging with KNN MSE: {mse_knn}\")\n",
        "\n",
        "# Compare performance\n",
        "if mse_dt < mse_knn:\n",
        "    print(\"Bagging with Decision Tree performed better.\")\n",
        "else:\n",
        "    print(\"Bagging with KNN performed better.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv841aXPPcV9",
        "outputId": "5942262d-ade3-4a4d-e616-afd625ac097a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging with Decision Tree MSE: 7484.147276569565\n",
            "Bagging with KNN MSE: 14688.318331938306\n",
            "Bagging with Decision Tree performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.41 Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score."
      ],
      "metadata": {
        "id": "L0z6v6KUPiae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model using ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft-DmB-MeTjn",
        "outputId": "1e2c4358-474c-482a-8f53-0150a38f70d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9952505732066819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.42 Train a Bagging Classifier and evaluate its performance using cross-validatio."
      ],
      "metadata": {
        "id": "mlgiQ7F0eY46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Create a Bagging Classifier\n",
        "bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
        "\n",
        "# Evaluate the Bagging Classifier using cross-validation\n",
        "cv_scores = cross_val_score(bagging_classifier, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print(\"Cross-validation scores:\", cv_scores)\n",
        "\n",
        "# Print the mean and standard deviation of the cross-validation scores\n",
        "print(f\"Mean Accuracy: {cv_scores.mean()}\")\n",
        "print(f\"Standard Deviation: {cv_scores.std()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMANFc3je7k7",
        "outputId": "e70e6d25-c5ea-4819-b162-fe1d676f165f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.95  0.9   0.895 0.895 0.86 ]\n",
            "Mean Accuracy: 0.9\n",
            "Standard Deviation: 0.028809720581775854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.43 Train a Random Forest Classifier and plot the Precision-Recall curve."
      ],
      "metadata": {
        "id": "BePtipS4fDvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_pred_prob = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Calculate AUC for Precision-Recall curve\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.plot(recall, precision, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "dtKVN4AZfT9M",
        "outputId": "d6318bfd-96aa-4652-98e0-bbf414eb6739"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###que.44 Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy."
      ],
      "metadata": {
        "id": "M7Ok0xU3fcKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the base estimators\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(random_state=42))\n",
        "]\n",
        "\n",
        "# Define the meta-learner\n",
        "meta_learner = LogisticRegression(random_state=42)\n",
        "\n",
        "# Create the stacking classifier\n",
        "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=meta_learner)\n",
        "\n",
        "# Train the stacking classifier\n",
        "stacking_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_stacking = stacking_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the stacking classifier\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "print(f\"Accuracy of Stacking Classifier: {accuracy_stacking}\")\n",
        "\n",
        "# Train individual classifiers for comparison\n",
        "for name, estimator in estimators:\n",
        "    estimator.fit(X_train, y_train)\n",
        "    y_pred = estimator.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy of {name}: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUwidVnAfp7u",
        "outputId": "547746fd-bebe-4a06-9063-c8de14b76b02"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Stacking Classifier: 0.89\n",
            "Accuracy of rf: 0.9\n",
            "Accuracy of lr: 0.855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUE.45 Train a Bagging Regressor with different levels of bootstrap samples and compare performance."
      ],
      "metadata": {
        "id": "-V5Q0_z8fwQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Generate a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bootstrap sample ratios to try\n",
        "bootstrap_ratios = [0.5, 0.7, 0.9, 1.0]\n",
        "\n",
        "for ratio in bootstrap_ratios:\n",
        "    # Create a Bagging Regressor\n",
        "    bagging_regressor = BaggingRegressor(\n",
        "        estimator=DecisionTreeRegressor(),\n",
        "        n_estimators=10,\n",
        "        max_samples=ratio,  # Set the bootstrap ratio\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the Bagging Regressor\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "    # Evaluate MSE\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"MSE with bootstrap ratio {ratio}: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auk9pqR_f-ZJ",
        "outputId": "8f48c46e-e853-45ca-e39c-59a301544740"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE with bootstrap ratio 0.5: 9103.131600547757\n",
            "MSE with bootstrap ratio 0.7: 8296.725506026647\n",
            "MSE with bootstrap ratio 0.9: 8687.871066191065\n",
            "MSE with bootstrap ratio 1.0: 7484.147276569565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#THE END"
      ],
      "metadata": {
        "id": "hzvI9ZpTgSKw"
      }
    }
  ]
}